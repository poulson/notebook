---
layout: post
title: Elemental's status as of version 0.85
date: 2014-11-10 07:42:00
location: Stanford (Stanford, CA)
author:
    twitter: libelemental
categories: notes
---

<img src="{{ site.baseurl }}/assets/elemental.png">

More than five months had passed since the last release of Elemental, and so 
[version 0.85 of the library](http://libelemental.org/releases/0.85/) was long 
overdue. In addition to a number of bug fixes and more consistent naming 
conventions, a number of significant features were added, such as sparse-direct 
functionality, generalized interfaces to distributed-memory functionality via 
read/write proxies, and nearly-complete C and python interfaces.

### What\'s new

#### Sparse-direct functionality

For the past two years, the sparse-direct functionality built on top of Elemental's 
distributed-memory dense linear algebra had existed within a separate project,
[Clique](https://github.com/poulson/Clique), which had been maintained much less
frequently than Elemental itself. For a laundry list of reasons, the functionality
in Clique was merged into Elemental and then subsequently improved and expanded
before the 0.85 release. For instance, while Clique supported sparse-direct multifrontal 
methods for Hermitian matrices, [^1] Elemental now also supports sparse-direct least-squares,
ridge, and Tikhonov regression. [^2]

[^1]: To be precise, Elemental currently restricts all pivots to be chosen within the supernodal 
      diagonal blocks, and so, while a large class of Hermitian matrices are supported, it is 
      straightforward to come up with a counter-example.

[^2]: For well-conditioned matrices, via the normal equations

#### Read/write proxies

Many routines in Elemental, such as the vast majority of its factorizations, 
are written under the assumption that the input matrix is distributed in the 
traditional 2D manner (i.e., for Elemental, the ``[MC,MR]`` distribution), 
and so a conversion mechanism is required if the interface to the routine is to
be general enough to accept any matrix distribution. In particular, there should
be essentially no overhead if the input matrix is already in an appropriate 
distribution.

Such a "zero-overhead" abstraction can be provided through the concept of a 
*read and/or write proxy*: if the distributed matrix is already of the correct 
form, the proxy can simply be a reference to the original matrix, but, 
otherwise, an initial redistribution must take place if the matrix is an input, 
and a final redistribution must take place if the matrix is an output.

Elemental implements the functions [``ReadProxy<T,U,V>``, ``WriteProxy<T,U,V>``, 
and ``ReadWriteProxy<T,U,V>``](https://github.com/elemental/Elemental/blob/5dc20f1424206f2a09b001e2585fe5c794e60dbf/include/El/core/Proxy.hpp) 
such that they return an instance of an ``std::shared_ptr<DistMatrix<T,U,V>>`` 
which, if appropriate, performs a redistribution in its constructor, and, if 
appropriate, performs another redistribution in its destructor (furthermore, 
many conversions of the underlying datatype are also supported).

Thus, if a routine expects an ``[MC,MR]`` distribution as an input/output, but a
general interface is desired, then the usage may look as follows:

{% highlight C++ %}
template<typename F>
void LowerCholesky( AbstractDistMatrix<F>& APre )
{
    auto APtr = ReadWriteProxy<F,MC,MR>( &APre );
    auto& A = *APtr;

    // Implement remainder of routine as if the input argument was
    // "DistMatrix<F,MC,MR>& A" instead of "AbstractDistMatrix<F>& APre"
}
{% endhighlight %}

#### C interface

While the first external language interface to Elemental was a 
[SWIG](https://github.com/swig/swig)-generated Python interface contributed
by [Michael Grant](http://cvxr.com/bio/), it soon became clear that SWIG 
would not be able to continue to parse Elemental's header files as Elemental
began to make use of C++11. For this reason, as well as the fact that 
several of Elemental's factorizations are wrapped by the C library
[PETSc](http://www.mcs.anl.gov/petsc/), and that C provides a significantly 
more consistent ABI than C++, the decision was made to create a high-quality C 
interface to the library and to use it as the lingua franca for building 
interfaces to other libraries.

While the vast majority of the library is currently exposed via a C interface 
which is new to version 0.85, the documentation for the C interface is lagging
significantly behind. However, a brief example is provided below:

{% highlight C %}
#include "El.h"

int main( int argc, char* argv[] )
{
    ElError error = ElInitialize( &argc, &argv );
    if( error != EL_SUCCESS )
        MPI_Abort( MPI_COMM_WORLD, 1 );

    ElInt m, n;
    error = ElInput_I("--m","matrix height",100,&m);
    EL_ABORT_ON_ERROR( error );
    error = ElInput_I("--n","matrix width",100,&n);
    EL_ABORT_ON_ERROR( error );

    ElGrid grid;
    error = ElDefaultGrid( &grid );
    EL_ABORT_ON_ERROR( error );

    ElDistMatrix_d A;
    error = ElDistMatrixCreate_d( grid, &A );
    EL_ABORT_ON_ERROR( error );
    error = ElDistMatrixResize_d( A, m, n );
    EL_ABORT_ON_ERROR( error );
    ElInt i, j;
    for( j=0; j<n; ++j )
    {
        for( i=0; i<m; ++i )
        {
            error = ElDistMatrixSet_d( A, i, j, (double)i-j );
            EL_ABORT_ON_ERROR( error );
        }
    }
    
    error = ElDisplayDist_d( A, "A(i,j) = i-j" );
    EL_ABORT_ON_ERROR( error );

    error = ElFinalize();
    if( error != EL_SUCCESS )
        MPI_Abort( MPI_COMM_WORLD, 1 );
    return 0;
}
{% endhighlight %}

#### ![Python]({{ site.baseurl }}/assets/python-small.png) interface

As mentioned above, Elemental's C interface is meant to be used as an 
intermediate when building interfaces to other languages, and so 
Elemental's new Python interface makes use of 
[ctypes](https://docs.python.org/2/library/ctypes.html) in order to build
an object-oriented (hopefully idiomatic) Python interface on top of the 
"flat" C interface. An example of constructing a distributed (sparse) 
negative 2D Laplacian vertically concatenated with an identity matrix and
then solving a least squares problem against a random right-hand side 
is demonstrated below:

{% highlight python %}
import El
n0 = n1 = 20

def ExtendedLaplacian(xSize,ySize):
  A = El.DistSparseMatrix()
  A.Resize(2*xSize*ySize,xSize*ySize)
  firstLocalRow = A.FirstLocalRow()
  localHeight = A.LocalHeight()
  A.Reserve(5*localHeight)
  hxInvSq = (1.*(xSize+1))**2
  hyInvSq = (1.*(ySize+1))**2
  for sLoc in xrange(localHeight):
    s = firstLocalRow + sLoc
    if s < xSize*ySize:
      x = s % xSize
      y = s / xSize
      A.QueueLocalUpdate( sLoc, s, 2*(hxInvSq+hyInvSq) )
      if x != 0:       A.QueueLocalUpdate( sLoc, s-1,     -hxInvSq )
      if x != xSize-1: A.QueueLocalUpdate( sLoc, s+1,     -hxInvSq )
      if y != 0:       A.QueueLocalUpdate( sLoc, s-xSize, -hyInvSq )
      if y != ySize-1: A.QueueLocalUpdate( sLoc, s+xSize, -hyInvSq )
    else:
      A.QueueLocalUpdate( sLoc, s-xSize*ySize, 2*(hxInvSq+hyInvSq) )

  A.MakeConsistent()
  return A

# Set up the linear operator as the vertical concatenation of a negative
# 2D Laplacian and an identity matrix.
A = ExtendedLaplacian(n0,n1)
El.Display( A, "A" )
El.Display( A.DistGraph(), "Graph of A" )

# Form the right-hand side as a uniformly-sampled distributed vector
y = El.DistMultiVec()
El.Uniform( y, 2*n0*n1, 1 )
El.Display( y, "y" )

# Solve the least-squares problem and display the relative error norm
yNrm = El.Nrm2(y)
x = El.LeastSquares(A,y)
El.SparseMultiply(El.NORMAL,-1,A,x,1,y)
El.Display(y,"A x - y")
eNrm = El.Nrm2(y)
if El.mpi.WorldRank() == 0:
  print "|| A x - y ||_2 / || y ||_2 = ", eNrm/yNrm
{% endhighlight %}

### Where to go from here?

#### Sparse Alternating Direction Method of Multipliers

While Elemental already contains a number of simple adaptations of 
the [ADMM example drivers](http://web.stanford.edu/~boyd/papers/admm/) provided
alongside the paper [Distributed optimization and statistical learning via the 
Alternating Direction Method of 
Multipliers](http://web.stanford.edu/~boyd/papers/admm_distr_stats.html),
many of the current implementations do not make use of robust stopping criteria 
and treat all input matrices as dense.

Given that Elemental now supports both sparse-direct Cholesky and least squares
problems, which tend to be the most significant component of several ADMM 
variants, it is now appropriate to modify the current implementations to make use
of this new functionality (as well as to improve convergence checks). This will
likely be a collaboration with [AJ Friend](http://stanford.edu/~ajfriend/) and
[Stephen Boyd](http://stanford.edu/~boyd/).

#### Interior Point Methods

Since Elemental contains, to the best of the author's knowledge, the only 
open-source distributed-memory Bunch-Kaufman implementation (as well as an 
extension to a sparse-direct solver with intranodal pivoting), it is natural
to use this solver within each step of a distributed Interior Point Method 
(IPM).
For example, [SDPARA](http://sdpa.sourceforge.net/download.html) either uses
[ScaLAPACK's](http://netlib.org/scalapack) dense or 
[MUMPS's](http://mumps.enseeiht.fr/) sparse-direct Cholesky factorization to 
handle the Schur complement rather than the entire Newton update.

Of course, an IPM should be typically be avoided in favor of first-order methods
(e.g., ADMM) when only modest-accuracy solutions are needed and representative
experiments have shown sufficient convergence rates.

#### 2D (and beyond) sparse matrix application

For sparse matrices whose graphs have highly variable edge degrees, it is 
well-known that the 1D sparse matrix distributions traditionally used by 
distributed-memory PDE solvers can easily be limited to $$ O(1) $$ speedup
despite the number of available processes. Thus, [BuluÃ§ et al.](http://hpc.sagepub.com/content/25/4/496.abstract) took a queue 
from distributed-memory dense linear algebra and proposed the usage of 
2D sparse matrix distributions as part of the [Combinatorial BLAS](http://gauss.cs.ucsb.edu/~aydin/CombBLAS/html/index.html), which is the backend for their 
[Knowledge Discovery Toolbox](http://kdt.sourceforge.net/wiki/index.php/Main_Page), which, in the worst case, guarantee roughly an $$ O(\sqrt{p}) $$ parallel
speedup when $$ p $$ processes are available.

It is planned for Elemental to transition from 1D to 2D sparse matrix 
distributions in the near future and, if possible, to investigate alternative
data distributions which might guarantee closer to $$ O(p) $$ worst-case
parallel speedups (e.g., through decomposing the original sparse matrix as 
sums and/or products of easily-applied matrices).

#### ![Julia]({{ site.baseurl }}/assets/julia-small.png) integration

Given that Elemental's Python interface is built on top of Elemental's C 
interface using [ctypes](https://docs.python.org/2/library/ctypes.html), an
analogous strategy should be the default approach to exposing a 
[Julia](http://julialang.org/) interface.

Another goal is to experiment with the performance and productivity tradeoffs 
that would result from natively implementing/extending Elemental's DistMatrix
class within Julia.

#### [IPython](http://ipython.org/)/[IJulia](https://github.com/JuliaLang/IJulia.jl)/[Jupyter](http://jupyter.org/) integration

Due to the large number of additional conveniences and the increased pedagogical
value of [IPython notebooks](http://ipython.org/) over a vanilla Python
shell (in particular, the improved support for 
[parallel computing](http://ipython.org/ipython-doc/stable/parallel/parallel_intro.html)), it would be worthwhile to perform any necessary steps for 
transitioning Elemental's current Python interface and examples over to 
IPython. Furthermore, as a Julia interface comes online, support for 
[IJulia](https://github.com/JuliaLang/IJulia.jl) (or, as it is soon to be 
renamed, [Jupyter](https://github.com/JuliaLang/IJulia.jl)) should also be 
added.

#### [Intel MKL](https://software.intel.com/en-us/intel-mkl) integration

There is currently discussion of the possibility of incorporating Elemental 
into [Intel's MKL](https://software.intel.com/en-us/intel-mkl). Though there is 
not yet a concrete plan, making Elemental 
available by default on the vast majority of high-performance would avoid
requiring most users from spending a significant amount of time ensuring that
the library is properly configured and therefore also accelerate adoption.
I will be giving a talk at one of Intel's Sunnyvale offices over the next few
weeks and will hopefully be able to say more soon.

#### Dedicated development machine

Elemental has been primarily developed on laptops and is only periodically 
performance-tested on large clusters. There is currently an effort to procure
a development machine in order to provide an easier on-ramp for interested
developers. If a large enough machine is acquired, Elemental will be regularly
publicly benchmarked on it so that users can have a rough idea how well the 
library should perform on their own machines.

#### Accelerator support

While *very* early versions of Elemental had (successfully) experimented with 
accelerator support by simply wrapping all local GEMM calls with a simple test 
for whether or not enough flops were being performed for the transfer cost to 
and from the GPU to be worthwhile, such functionality was never fully fleshed 
out or made part of an official release. Accelerator support is now long 
overdue, and, in fact, the most frequent request any time the project is 
described to a general audience.

#### Optional usage of [BLIS](https://github.com/flame/blis)

It is frequently the case that large portions of the local computations within 
distributed-memory dense linear algebra do not optimally map to the standard
[BLAS](http://netlib.org/blas) API (see, for example, the need for 
[two fused TRMV's described by Sears et al.](http://dl.acm.org/citation.cfm?id=509111)), 
and [BLIS](https://github.com/flame/blis) is a natural candidate for how to 
portably generalize BLAS.

#### Making naming conventions consistent with scientific computing ecosystem

Though Elemental has historically made use of upper camel case for classes and 
functions, (lower) camel case for variable names, and 
UPPER_CASE_WITH_UNDERSCORES for constants and macros, these conventions are at
odds with the C++, python, and Julia communities at large. If members of 
Elemental's community continue to vocally oppose the current style, the issue
may be revisited in the future.

*Availability*: The source code for Elemental can be downloaded from either
the project website, [libelemental.org](http://libelemental.org) or from
[GitHub](https://github.com/elemental/Elemental.git) and is made available
under the [New BSD License](http://opensource.org/licenses/BSD-3-Clause).
